import streamlit as st
import torch
import json
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from torch.utils.data import Dataset, DataLoader
import os

# è¨­å®šæ¨¡å‹ä¿å­˜è·¯å¾‘
MODEL_SAVE_PATH = "fine_tuned_gpt2"

# è¼‰å…¥æˆ–åˆå§‹åŒ–æ¨¡å‹å’Œ tokenizer  
@st.cache_resource
def load_model():
    if os.path.exists(MODEL_SAVE_PATH):
        tokenizer = GPT2Tokenizer.from_pretrained(MODEL_SAVE_PATH)
        model = GPT2LMHeadModel.from_pretrained(MODEL_SAVE_PATH)
    else:
        tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
        model = GPT2LMHeadModel.from_pretrained("gpt2")
        
    # è¨­ç½®å¡«å……ç¬¦è™Ÿï¼ˆpad_tokenï¼‰ï¼Œå¦‚æœæ²’æœ‰å°±ä½¿ç”¨ eos_token æˆ–è‡ªå®šç¾©å¡«å……ç¬¦è™Ÿ
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token  # ä½¿ç”¨ eos_token ä¾†ä½œç‚ºå¡«å……ç¬¦è™Ÿ
    
    return tokenizer, model

tokenizer, model = load_model()

# è‡ªå®šç¾© Dataset é¡åˆ¥
class TextDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings["input_ids"])

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.encodings.items()}

# è¨“ç·´æ¨¡å‹ï¼ˆæ”¯æŒ JSON æˆ– TXTï¼‰
def train_model(training_data, file_type):
    # æª¢æŸ¥æ•¸æ“šæ ¼å¼
    if not training_data:
        st.error("âŒ è¨“ç·´æ•¸æ“šç‚ºç©ºï¼")
        return

    # è§£ææ•¸æ“š
    texts = []
    if file_type == "json":
        try:
            data = json.loads(training_data)
            st.write(f"âœ… JSON åŠ è¼‰æˆåŠŸ: {data}")  # Debug
            if not isinstance(data, list):
                st.error("âŒ JSON æ ¼å¼éŒ¯èª¤ï¼Œæ‡‰è©²æ˜¯åˆ—è¡¨")
                return
            texts = [item["text"] for item in data if "text" in item]
        except Exception as e:
            st.error(f"âŒ JSON è§£æéŒ¯èª¤ï¼š{e}")
            return
    elif file_type == "txt":
        texts = [line.strip() for line in training_data.split("\n") if line.strip()]

    # Debugï¼šç¢ºä¿ texts æ­£ç¢º
    st.write(f"ğŸ“Š è™•ç†å¾Œçš„æ–‡æœ¬æ•¸é‡ï¼š{len(texts)}")

    if not texts:
        st.error("âŒ æ²’æœ‰æœ‰æ•ˆçš„æ–‡æœ¬æ•¸æ“šï¼")
        return

    # å˜—è©¦ Tokenizationï¼Œå¢åŠ éŒ¯èª¤è™•ç†
    try:
        inputs = tokenizer(
            texts,
            return_tensors="pt",
            truncation=True,
            padding=True,  # ç¢ºä¿å¡«å……
            max_length=256,  # é¿å…éé•·
            add_special_tokens=True
        )
    except Exception as e:
        st.error(f"âŒ Tokenization å¤±æ•—: {e}")
        return

    # å‰µå»º Dataset å’Œ DataLoader
    dataset = TextDataset(inputs)
    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

    # è¨“ç·´åƒæ•¸
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
    epochs = 3
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.train()

    # è¨“ç·´å¾ªç’°
    for epoch in range(epochs):
        total_loss = 0
        for batch in dataloader:
            optimizer.zero_grad()
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        st.write(f"âœ… Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}")

    # ä¿å­˜æ¨¡å‹
    model.save_pretrained(MODEL_SAVE_PATH)
    tokenizer.save_pretrained(MODEL_SAVE_PATH)
    st.success("ğŸ‰ æ¨¡å‹è¨“ç·´å®Œæˆï¼")

# ç”Ÿæˆæ–‡æœ¬
def generate_text(prompt):
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")
    output = model.generate(input_ids, max_length=100, num_return_sequences=1)
    return tokenizer.decode(output[0], skip_special_tokens=True)

# Streamlit UI
st.title("ğŸ¤– GPT-2 è¨“ç·´æ‡‰ç”¨")

# å´é‚Šæ¬„é¸å–®
option = st.sidebar.selectbox(
    "ğŸ” é¸æ“‡åŠŸèƒ½",
    ["èˆ‡ AI å°è©±", "ä¸Šå‚³è¨“ç·´æª”æ¡ˆ", "ä¸‹è¼‰æ¨¡å‹"]
)

if option == "èˆ‡ AI å°è©±":
    st.header("ğŸ—£ï¸ èˆ‡ AI å°è©±")
    user_input = st.text_input("ğŸ”¹ è¼¸å…¥æç¤ºï¼š")
    if st.button("ğŸš€ ç”Ÿæˆ"):
        if user_input:
            response = generate_text(user_input)
            st.write("ğŸ§  AI å›æ‡‰ï¼š")
            st.write(response)
        else:
            st.warning("âš ï¸ è«‹è¼¸å…¥å…§å®¹ï¼")

elif option == "ä¸Šå‚³è¨“ç·´æª”æ¡ˆ":
    st.header("ğŸ“‚ ä¸Šå‚³è¨“ç·´æª”æ¡ˆ")
    uploaded_file = st.file_uploader("ğŸ“¥ é¸æ“‡æª”æ¡ˆï¼ˆJSON æˆ– TXTï¼‰", type=["json", "txt"])
    if uploaded_file:
        file_type = uploaded_file.type.split("/")[-1]
        content = uploaded_file.read().decode("utf-8")
        st.write("âœ… æª”æ¡ˆè§£ææˆåŠŸï¼")
        if st.button("ğŸš€ é–‹å§‹è¨“ç·´"):
            train_model(content, file_type)

elif option == "ä¸‹è¼‰æ¨¡å‹":
    st.header("ğŸ“¥ ä¸‹è¼‰æ¨¡å‹")
    if os.path.exists(MODEL_SAVE_PATH):
        st.download_button(
            "â¬‡ï¸ ä¸‹è¼‰æ¨¡å‹æ¬Šé‡",
            data=open(os.path.join(MODEL_SAVE_PATH, "pytorch_model.bin"), "rb"),
            file_name="gpt2_finetuned.bin",
            mime="application/octet-stream"
        )
    else:
        st.warning("âš ï¸ å°šæœªè¨“ç·´æ¨¡å‹ï¼")
